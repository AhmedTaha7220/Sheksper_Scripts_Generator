{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### â€¢\tYou need a transformer to build the single word or a token this is a sub-transformer, and you need another transformer model to build a sequence of words through the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kubrart calls and hyper parameters\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 32 # how many independent sequences will we process in parallel in tran encoder & decoder?\n",
    "\n",
    "block_size = 8 # what is the maximum context length for predictions? After predicting this number of characters we should truncate and start from the beginning\n",
    "# as the trans you train can't predict more than that\n",
    "\n",
    "max_iters = 3000\n",
    "eval_interval = 300\n",
    "\n",
    "# It's the learning rate for the model\n",
    "learning_rate = 1e-2\n",
    "\n",
    "# It's for enabling GPU when running the model to make things fater and of course that 'If' you have a GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading training data\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding & Decoding words:\n",
    "Notice here We are going to use our self method to encode words but of course there are many ready methods that you can use to encode your words such as \"tiktokens\" used by GPT, \"Sentence Piece\" in google, and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 47, 1, 58, 46, 47, 57, 1, 47, 57, 1, 13, 46, 51, 43, 42]\n",
      "Hi this is Ahmed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "\n",
    "# Length of all characters after separating the characters of the text in a list\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# create a mapping from characters to integers and versa\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# 'stoi': it's a function that's used to convert each character in the string into an integer number\n",
    "# to perform the embding operation so each word will be represented as list of numbers \n",
    "# Its syntax is (stoi(['character']))\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"Hi this is Ahmed\"))\n",
    "print(decode(encode(\"Hi this is Ahmed\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passing the training data after decoding it to 'Tensor' to apply training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype) # Printing info about data\n",
    "print(data[:1000]) # Printing first 1000 encoded characters from data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting our data into 'tarining' and 'testing' data\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When Input is tensor([18]) The output should be 47\n",
      "When Input is tensor([18, 47]) The output should be 56\n",
      "When Input is tensor([18, 47, 56]) The output should be 57\n",
      "When Input is tensor([18, 47, 56, 57]) The output should be 58\n",
      "When Input is tensor([18, 47, 56, 57, 58]) The output should be 1\n",
      "When Input is tensor([18, 47, 56, 57, 58,  1]) The output should be 15\n",
      "When Input is tensor([18, 47, 56, 57, 58,  1, 15]) The output should be 47\n",
      "When Input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) The output should be 58\n",
      "When Input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58]) The output should be 47\n",
      "When Input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47]) The output should be 64\n",
      "When Input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64]) The output should be 43\n",
      "When Input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43]) The output should be 52\n",
      "When Input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52]) The output should be 10\n",
      "When Input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10]) The output should be 0\n",
      "When Input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0]) The output should be 14\n",
      "When Input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14]) The output should be 43\n",
      "When Input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43]) The output should be 44\n",
      "When Input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44]) The output should be 53\n",
      "When Input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53]) The output should be 56\n",
      "When Input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56]) The output should be 43\n",
      "When Input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43]) The output should be 1\n",
      "When Input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1]) The output should be 61\n",
      "When Input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61]) The output should be 43\n",
      "When Input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43]) The output should be 1\n",
      "When Input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1]) The output should be 54\n",
      "When Input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54]) The output should be 56\n",
      "When Input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56]) The output should be 53\n",
      "When Input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53]) The output should be 41\n",
      "When Input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41]) The output should be 43\n",
      "When Input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43]) The output should be 43\n",
      "When Input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43]) The output should be 42\n",
      "When Input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42]) The output should be 1\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:batch_size]# It's the input for trans\n",
    "y = train_data[1:batch_size+1]# It's the output of trans that's why it's shifted by 1 from x as it indicates the next word after the input\n",
    "\n",
    "for char in range (batch_size):\n",
    "    context = x[:char+1] #It means print the sequence from the start to the character\n",
    "    target = y[char] # It means print the next character after the sequence\n",
    "    print(f\"When Input is {context} The output should be {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading 'Preparing our batch to enter it to the model'\n",
    "def get_batch(split):\n",
    "    \n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data # Here determining whether you are dealing with training or test data\n",
    "    \n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # Here we are taking a random blocks or chunks from our sequence of data\n",
    "    \n",
    "    # It's the input for trans \n",
    "    # we use 'stack' key word to faltten them in an one dimensional array\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) \n",
    "    # It's the output of trans that's why it's shifted by 1 from x as it indicates the next word after the input\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) \n",
    "    \n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing our data to fit the model 'BigramLanguageModel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        ''' \n",
    "        - The importance of this 'Embedding function':\n",
    "            It's going to take the target's index as the output then it will take its sequence or context as the input then it will create nn layers between\n",
    "            the inputs 'sequence' and the output 'target'\n",
    "        \n",
    "        - It takes 2D list of the targets predited from particular sequences and it produces a list has 3 information:\n",
    "            B 'Batch': it refers to number of distinct sequences enters the model\n",
    "            T 'Time': it refers to number of predictions of targets in each sequence\n",
    "            C 'Channel': it's the total size of vocab_size which are the length of the characters used in the text\n",
    "        '''\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        '''What's done here is that it will take each output\n",
    "        With its row that's used for predicting it and order them in form of (B,T,C) then this is the logit which is the score\n",
    "        of the prediction of the next character\n",
    "        NOTICE: (B,C,T) represents the details and info for each token to predit the next one Because unlike the recurrence model, in transfomer\n",
    "        each token is independent so from each token we should be able to predict the next one without depending on any other tokens that's why each\n",
    "        token must have all the details that we need to predict the next one and those details are (B, T, and C) for that token'''\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C) it means (Batch, Time, Channel)\n",
    "\n",
    "        # After calculating the logitis which is our prediction list for the next tokens we print its shape and compare it with the real target to \n",
    "        # see our score in predicitons\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Notice if we used the 3 results from logits which are (B,T,C) the cross_entropy produces an error as it expects only 2 parameters \n",
    "            # that's why we reshaped the coordinates of logits results in the next 3 statements\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            \n",
    "            # comparing between logits and targets using the score function 'cross_entropy'\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    # This function is used to generate the next tokens from a given list \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx) # it means apply the constructor of this class on idx list to produces the 3 info (B,T, and C)\n",
    "            \n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C) # It means takes all the elements except the last element because you can't predit what's after the last\n",
    "            \n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C) # Apply softmax on logits to know what's the maximum probability of tokens to generate\n",
    "            \n",
    "            # sample from the distribution\n",
    "             # It's used for obtaining only one probability which is the biggest one\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1) it means for each batch predict one value of the next token\n",
    "            \n",
    "            # append sampled index to the running sequence\n",
    "            # Here we concatenate the sequence with each output \n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1) \n",
    "        return idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training our model to avoid producing rubbish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3833913803100586\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)  \n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "@torch.no_grad()\n",
    "# no_grad: it's used for disabling gradients from updating while calcluating the evaluations\n",
    "\n",
    "# The main use of this function is averging the loss instead of looking for the optimum loss through multiple iterations so this function is directly average\n",
    "# the loss in the train and val data\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean() # This is the function that's used to calculate the average of the losses\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train') # sample a batch of data passing the input data to xb and the output data to yb\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb) # applying the model to training data returning the loss and logits\n",
    "    optimizer.zero_grad(set_to_none=True) # zeroing all the gradients from the previous steps\n",
    "    loss.backward() # Obtaining new gradients through backprobagation through loss function\n",
    "    optimizer.step() # Finally moves one step using optimizer\n",
    " \n",
    "for step in range (1000):\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb) \n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward() \n",
    "    optimizer.step() \n",
    "print(loss.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "DUTh gwin s t, pir,is al,\n",
      "Un f third y wer d w f hen w.\n",
      "\n",
      "Jonothove m, t ha g gelld?\n",
      "'?-dllerllle warrerse LO&ju b.\n",
      "\n",
      "DUMEne theomemThSmungucifajeve'shevoQUBe'Tyout:\n",
      "Corens mivind thee wilenss wia\n",
      "TAUCKints rel wie the.\n",
      "D:\n",
      "DUPEO;\n",
      "CUSCaiceaiaplstinge ICllel'ser d hy urwWhelo be\n",
      "\n",
      "\n",
      "Caleswhe\n",
      "\n",
      "I d be haVing:\n",
      "Bu quZI byMPUDUS:\n",
      "Cipr bjur:\n",
      "wingt t ook!\n",
      "T f f w\n",
      "HEDDWis e ldM\n",
      "Y Ianes\n",
      "Alla g iote ake an y$zely titQNor mpld,\n",
      "BRBur be oupg.\n",
      "WncerexAqureered?\n",
      "Gof cop uma I f anqustyoreiY-m.\n",
      "\n",
      "Nlsleat PThand fur\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
