{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important notes in transformers\n",
    "1. In the previous code 'bigram' every token was only using its information stored in 'Channel' variable to predict its next word, but wasn't write or accurate.\n",
    "\n",
    "2. Here we want to make kind of connection or communication between tokens to make each token use 2 things to predict the next tokens (its info stored in channel) and (info of the previous tokens 'channels of the previous tokens').\n",
    "\n",
    "3. 2 important notes First, each token should only have the access to the previous tokens not the future ones we should achieve that in our code, second to obtain all the information 'channels' of the previous tokens we calculate the average 'the mean' of those channels. ((Now this would like a feature vector includes the info of the current token and the previous channels mean.))\n",
    "\n",
    "= Come later:\n",
    "4. Although it's a good method to obtain info, BUT we missed very important feature which is the spatial arrangement of tokens and the order of the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.1711, -0.3062],\n",
      "        [-1.0709, -0.0416],\n",
      "        [ 0.8544, -0.9220],\n",
      "        [ 0.7695,  0.5506],\n",
      "        [-0.5017, -1.4585],\n",
      "        [-1.4429, -0.0798],\n",
      "        [-0.9312, -1.9201],\n",
      "        [-0.5831, -0.2524]])\n",
      "tensor([[ 1.1711, -0.3062],\n",
      "        [ 0.0501, -0.1739],\n",
      "        [ 0.3182, -0.4233],\n",
      "        [ 0.4310, -0.1798],\n",
      "        [ 0.2445, -0.4355],\n",
      "        [-0.0368, -0.3763],\n",
      "        [-0.1645, -0.5968],\n",
      "        [-0.2169, -0.5538]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThe difference between the output of x and xbow:\\n    - x container: holds random values \\n    - xbow: holds the average of each element and all its previous elements\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of what we want to do\n",
    "B,T,C = 4,8,2\n",
    "x = torch.randn(B,T,C) # Here we create a random array with dimensions B,T,C\n",
    "x.shape\n",
    "\n",
    "xbow = torch.zeros((B,T,C)) # xbow refers to bag of words\n",
    "\n",
    "for b in range(B):\n",
    "    for t in range (T):\n",
    "        xprev = x[b,:t+1] # Adding all the elements including the current element that I am standing on\n",
    "        xbow[b,t]=torch.mean(xprev, 0) # Calculating the average of them\n",
    "\n",
    "print(x[0])\n",
    "print(xbow[0])\n",
    "'''\n",
    "The difference between the output of x and xbow:\n",
    "    - x container: holds random values \n",
    "    - xbow: holds the average of each element and all its previous elements\n",
    "that's why the first element is equal in both containers BUT starting from the next element in xbow it calculates the average of the first 2 elements in x container\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The mathematical trick of average\n",
    "- If you multiplied any matrix by a (triangular ones averged matrix) the out will be the average of each element in the matrix \"the thing that we want to do with channels or tokens\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "tensor([[5., 6.],\n",
      "        [5., 1.],\n",
      "        [5., 4.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[5.0000, 6.0000],\n",
       "        [5.0000, 3.5000],\n",
       "        [5.0000, 3.6667]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparing our (triangular averaged ones matrix)\n",
    "a=torch.tril(torch.ones((3,3)))\n",
    "a = a/torch.sum(a,1,keepdim=True)\n",
    "\n",
    "# Creating our given matrix (the one that we want to get the average for)\n",
    "b=torch.randint(0,10,(3,2)).float()\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "# Multiplication to get the average \n",
    "c = a@b\n",
    "c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we want to use the same principle with the x matrix to create xbow matrix\n",
    "\n",
    "# Preparing ones matrix\n",
    "ones = torch.tril(torch.ones(T,T))\n",
    "ones = ones/torch.sum(ones,1,keepdim=True)\n",
    "\n",
    "xbow2 = ones@x\n",
    "\n",
    "# Notice the 2 matrices are matched which means the method is working \n",
    "torch.allclose(xbow,xbow2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
